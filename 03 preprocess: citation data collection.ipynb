{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51917508",
   "metadata": {},
   "source": [
    "**PHASE 1: DATA PREPROCESSING - Collect Citation Data**\n",
    "\n",
    "Purpose: Fetch citation counts from Semantic Scholar API  \n",
    "Input: arxiv_metadata_features.pkl  \n",
    "Output: citation_data_full.pkl (stratified sample, batched collection)  \n",
    "Strategy: 1M paper sample, 50K batches, progress saved automatically  \n",
    "ML Involved: None - Data collection via API  \n",
    "Runtime: ~1.5 hours per 50K papers (500K = 15 hrs, 1M = 30 hrs)  \n",
    "Note: Citations kept separate, merged later in analysis phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47559a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2,384,622 papers\n"
     ]
    }
   ],
   "source": [
    "# setup and load data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# load metadata\n",
    "df = pd.read_pickle('data/processed/arxiv_metadata_features.pkl')\n",
    "print(f\"Loaded: {len(df):,} papers\")\n",
    "\n",
    "# create citations folder\n",
    "os.makedirs('data/processed/citations_batches', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f737c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/h76_7dj53pd2_090j4pskdzm0000gn/T/ipykernel_6262/312093858.py:10: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sample = df.groupby(['year', 'top_level_domain'], group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled: 999,995 papers\n",
      "\n",
      "Year distribution:\n",
      "year\n",
      "2016     48376\n",
      "2017     49634\n",
      "2018     56243\n",
      "2019     63127\n",
      "2020     72433\n",
      "2021     77204\n",
      "2022     78464\n",
      "2023     89728\n",
      "2024    108887\n",
      "2025    102317\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Domain distribution:\n",
      "top_level_domain\n",
      "cs          269247\n",
      "math        206833\n",
      "cond-mat    106321\n",
      "astro-ph    102131\n",
      "physics      75531\n",
      "quant-ph     41671\n",
      "hep-ph       35975\n",
      "hep-th       27828\n",
      "eess         26331\n",
      "stat         23476\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Sample IDs saved\n"
     ]
    }
   ],
   "source": [
    "# target sample size\n",
    "\n",
    "TARGET_SAMPLE = 1_000_000  # ambitious but seeing if I can get 1m count\n",
    "\n",
    "# stratified sampling by year and top_level_domain\n",
    "# this ensures representation across time and fields\n",
    "\n",
    "sample_fraction = TARGET_SAMPLE / len(df)\n",
    "\n",
    "df_sample = df.groupby(['year', 'top_level_domain'], group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sample_fraction, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# shuffle randomly\n",
    "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Sampled: {len(df_sample):,} papers\")\n",
    "print(f\"\\nYear distribution:\")\n",
    "print(df_sample['year'].value_counts().sort_index().tail(10))\n",
    "print(f\"\\nDomain distribution:\")\n",
    "print(df_sample['top_level_domain'].value_counts().head(10))\n",
    "\n",
    "# save sample IDs for reference\n",
    "df_sample[['id', 'year', 'primary_category']].to_pickle(\n",
    "    'data/processed/citation_sample_ids.pkl'\n",
    ")\n",
    "print(\"\\n✓ Sample IDs saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a5e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing batches found. Starting fresh.\n"
     ]
    }
   ],
   "source": [
    "# check for existing batches\n",
    "\n",
    "batch_dir = 'data/processed/citations_batches'\n",
    "existing_batches = sorted([f for f in os.listdir(batch_dir) if f.startswith('batch_')])\n",
    "\n",
    "if existing_batches:\n",
    "    print(f\"Found {len(existing_batches)} existing batches:\")\n",
    "    \n",
    "    # load and combine existing batches\n",
    "    existing_dfs = []\n",
    "    for batch_file in existing_batches:\n",
    "        batch_df = pd.read_pickle(os.path.join(batch_dir, batch_file))\n",
    "        existing_dfs.append(batch_df)\n",
    "        print(f\"{batch_file}: {len(batch_df):,} papers\")\n",
    "    \n",
    "    df_existing = pd.concat(existing_dfs, ignore_index=True)\n",
    "    already_collected = set(df_existing['id'].values)\n",
    "    print(f\"\\nTotal collected: {len(already_collected):,} papers\")\n",
    "    \n",
    "    # filter sample to exclude already collected citation papers\n",
    "    df_sample = df_sample[~df_sample['id'].isin(already_collected)].reset_index(drop=True)\n",
    "    print(f\"Remaining to collect: {len(df_sample):,} papers\")\n",
    "else:\n",
    "    print(\"No existing batches found. Starting fresh.\")\n",
    "    already_collected = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464f7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_citations_batch(arxiv_ids, batch_name, start_idx=0):\n",
    "    \"\"\"\n",
    "    fetch citations for a list of ArXiv IDs\n",
    "    saves progress every 1000 papers\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    errors = 0\n",
    "    rate_limits = 0\n",
    "    \n",
    "    print(f\"\\nFetching batch: {batch_name}\")\n",
    "    print(f\"Papers to fetch: {len(arxiv_ids)}\")\n",
    "    \n",
    "    for i, arxiv_id in enumerate(tqdm(arxiv_ids, desc=batch_name)):\n",
    "        try:\n",
    "            # semantic Scholar API\n",
    "            url = f\"https://api.semanticscholar.org/graph/v1/paper/ARXIV:{arxiv_id}\"\n",
    "            params = {\"fields\": \"citationCount,year\"}\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                results.append({\n",
    "                    'id': arxiv_id,\n",
    "                    'citation_count': data.get('citationCount', 0),\n",
    "                    'ss_year': data.get('year', None)\n",
    "                })\n",
    "            elif response.status_code == 429:\n",
    "                # rate limited - wait and retry\n",
    "                rate_limits += 1\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                errors += 1\n",
    "                results.append({\n",
    "                    'id': arxiv_id,\n",
    "                    'citation_count': 0,\n",
    "                    'ss_year': None\n",
    "                })\n",
    "            \n",
    "            # small delay to respect API\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            # save temporary checkpoint every 1000 papers\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                temp_df = pd.DataFrame(results)\n",
    "                temp_df.to_pickle(f'data/processed/citations_batches/{batch_name}_temp.pkl')\n",
    "        \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            results.append({\n",
    "                'id': arxiv_id,\n",
    "                'citation_count': 0,\n",
    "                'ss_year': None\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    # create final dataframe\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # save batch\n",
    "    df_results.to_pickle(f'data/processed/citations_batches/{batch_name}.pkl')\n",
    "    \n",
    "    # remove temp file if exists\n",
    "    temp_file = f'data/processed/citations_batches/{batch_name}_temp.pkl'\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "    \n",
    "    print(f\"\\n✓ Batch complete!\")\n",
    "    print(f\"  Successful: {len(results) - errors:,}\")\n",
    "    print(f\"  Errors: {errors:,}\")\n",
    "    print(f\"  Rate limits hit: {rate_limits:,}\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694ca111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will collect 10 batches of 50,000 papers\n",
      "Total target: 500,000 papers\n",
      "Estimated time: 15.0 hours at 33K/hour\n",
      "\n",
      "✓ df_sample exists: 999,995 papers\n",
      "✓ BATCH_SIZE: 50,000\n",
      "✓ MAX_BATCHES: 10\n",
      "BATCH 1/10\n",
      "Papers 0 to 50,000\n",
      "\n",
      "Fetching batch: batch_00\n",
      "Papers to fetch: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_00:  17%|█▋        | 8560/50000 [9:43:50<47:06:27,  4.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPapers \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# fetch citations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m df_batch = \u001b[43mfetch_citations_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Saved: data/processed/citations_batches/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_sample)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m papers\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mfetch_citations_batch\u001b[39m\u001b[34m(arxiv_ids, batch_name, start_idx)\u001b[39m\n\u001b[32m     16\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.semanticscholar.org/graph/v1/paper/ARXIV:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marxiv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m params = {\u001b[33m\"\u001b[39m\u001b[33mfields\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcitationCount,year\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     22\u001b[39m     data = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "\n",
    "BATCH_SIZE = 50_000  # papers per batch\n",
    "MAX_BATCHES = 10      # 10 batches = 500K papers (to start, will later change to 20 for full 1M if all is well)\n",
    "\n",
    "# calculate batches\n",
    "num_batches = min(MAX_BATCHES, (len(df_sample) // BATCH_SIZE) + 1)\n",
    "\n",
    "print(f\"Will collect {num_batches} batches of {BATCH_SIZE:,} papers\")\n",
    "print(f\"Total target: {min(num_batches * BATCH_SIZE, len(df_sample)):,} papers\")\n",
    "print(f\"Estimated time: {(num_batches * 1.5):.1f} hours at 33K/hour\\n\")\n",
    "\n",
    "# verify everything is loaded before starting\n",
    "\n",
    "try:\n",
    "    print(f\"✓ df_sample exists: {len(df_sample):,} papers\")\n",
    "    print(f\"✓ BATCH_SIZE: {BATCH_SIZE:,}\")\n",
    "    print(f\"✓ MAX_BATCHES: {MAX_BATCHES}\")\n",
    "except NameError as e:\n",
    "    print(f\"x ERROR: {e}\")\n",
    "    print(\"Run cells 1-4 first!\")\n",
    "    raise\n",
    "\n",
    "# collect each batch\n",
    "for batch_num in range(num_batches):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(df_sample))\n",
    "    \n",
    "    batch_ids = df_sample['id'].iloc[start_idx:end_idx].tolist()\n",
    "    batch_name = f\"batch_{batch_num:02d}\"\n",
    "    \n",
    "    # skip if batch citation pull already exists\n",
    "    if os.path.exists(f'data/processed/citations_batches/{batch_name}.pkl'):\n",
    "        print(f\"\\n✓ {batch_name} already exists, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"BATCH {batch_num + 1}/{num_batches}\")\n",
    "    print(f\"Papers {start_idx:,} to {end_idx:,}\")\n",
    "    \n",
    "    # fetch citations\n",
    "    df_batch = fetch_citations_batch(batch_ids, batch_name)\n",
    "    \n",
    "    print(f\"\\n✓ Saved: data/processed/citations_batches/{batch_name}.pkl\")\n",
    "    print(f\"Progress: {end_idx:,} / {len(df_sample):,} papers\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"All batches complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all batches into single file\n",
    "\n",
    "batch_dir = 'data/processed/citations_batches'\n",
    "batch_files = sorted([f for f in os.listdir(batch_dir) if f.startswith('batch_') and not f.endswith('_temp.pkl')])\n",
    "\n",
    "print(f\"Combining {len(batch_files)} batches...\")\n",
    "\n",
    "all_batches = []\n",
    "for batch_file in batch_files:\n",
    "    df_batch = pd.read_pickle(os.path.join(batch_dir, batch_file))\n",
    "    all_batches.append(df_batch)\n",
    "    print(f\"  {batch_file}: {len(df_batch):,} papers\")\n",
    "\n",
    "df_citations = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal citations collected: {len(df_citations):,}\")\n",
    "print(f\"Papers with citations > 0: {(df_citations['citation_count'] > 0).sum():,}\")\n",
    "print(f\"Average citations: {df_citations['citation_count'].mean():.1f}\")\n",
    "\n",
    "# save combined file\n",
    "\n",
    "df_citations.to_pickle('data/processed/citation_data_full.pkl')\n",
    "print(\"\\n✓ Saved: data/processed/citation_data_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# check final file\n",
    "\n",
    "if os.path.exists('data/processed/citation_data_full.pkl'):\n",
    "    df_check = pd.read_pickle('data/processed/citation_data_full.pkl')\n",
    "    size_mb = os.path.getsize('data/processed/citation_data_full.pkl') / 1024**2\n",
    "    \n",
    "    print(\"✓✓✓ Success! ✓✓✓\")\n",
    "    print(f\"Citations collected: {len(df_check):,}\")\n",
    "    print(f\"File size: {size_mb:.1f} MB\")\n",
    "    print(f\"\\nCitation stats:\")\n",
    "    print(df_check['citation_count'].describe())\n",
    "else:\n",
    "    print(\"x File not created yet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
