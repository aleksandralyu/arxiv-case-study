{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5557f86",
   "metadata": {},
   "source": [
    "**PHASE 2: FEATURE ENGINEERING - Dimensionality Reduction (PCA)**\n",
    "\n",
    "Purpose: Reduce 10K features → 100 dimensions while preserving variance  \n",
    "Input: tfidf_matrix.pkl (2.4M × 10K)  \n",
    "Output: pca_reduced_100d.pkl (2.4M × 100), pca_model.pkl  \n",
    "Algorithm: Incremental PCA (designed for large datasets)  \n",
    "Target: Preserve 70-80% of variance in 100 components  \n",
    "ML Involved: YES - PCA is unsupervised dimensionality reduction  \n",
    "Runtime: ~15-20 minutes  \n",
    "Why: Makes clustering feasible, reduces noise, avoids curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240324b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports loaded\n",
      "PCA Config: n_components=100, batch_size=10000\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add project root to path for config\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import PCA_N_COMPONENTS, PCA_BATCH_SIZE, RANDOM_STATE\n",
    "\n",
    "print(\"✓ All imports loaded\")\n",
    "print(f\"PCA Config: n_components={PCA_N_COMPONENTS}, batch_size={PCA_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b46e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF-IDF matrix...\n",
      "✓ Loaded TF-IDF matrix\n",
      "  Shape: (2384622, 3000)\n",
      "  Papers: 2,384,622\n",
      "  Features: 3,000\n",
      "\n",
      "✓ Loaded paper IDs: 2,384,622\n"
     ]
    }
   ],
   "source": [
    "# load TF-IDF matrix and metadata\n",
    "\n",
    "print(\"Loading TF-IDF matrix...\")\n",
    "tfidf_matrix = joblib.load('data/processed/tfidf_matrix.pkl')\n",
    "print(f\"✓ Loaded TF-IDF matrix\")\n",
    "print(f\"  Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"  Papers: {tfidf_matrix.shape[0]:,}\")\n",
    "print(f\"  Features: {tfidf_matrix.shape[1]:,}\")\n",
    "\n",
    "# load paper IDs to maintain alignment\n",
    "\n",
    "df_ids = pd.read_pickle('data/processed/tfidf_paper_ids.pkl')\n",
    "print(f\"\\n✓ Loaded paper IDs: {len(df_ids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241b5fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Incremental PCA initialized\n",
      "  Target components: 100\n",
      "  Batch size: 10,000 papers per batch\n",
      "  Total batches: 239\n"
     ]
    }
   ],
   "source": [
    "# initialize pca\n",
    "# using Incremental PCA because we have a large datasets\n",
    "# processes data in batches instead of loading all into memory\n",
    "\n",
    "pca = IncrementalPCA(\n",
    "    n_components=PCA_N_COMPONENTS,\n",
    "    batch_size=PCA_BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"✓ Incremental PCA initialized\")\n",
    "print(f\"  Target components: {PCA_N_COMPONENTS}\")\n",
    "print(f\"  Batch size: {PCA_BATCH_SIZE:,} papers per batch\")\n",
    "print(f\"  Total batches: {int(np.ceil(tfidf_matrix.shape[0] / PCA_BATCH_SIZE)):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9585e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Incremental PCA...\n",
      "This will take 3-4 hours...\n",
      "\n",
      "  Processed batch 10/239 (100,000/2,384,622 papers)\n",
      "  Processed batch 20/239 (200,000/2,384,622 papers)\n",
      "  Processed batch 30/239 (300,000/2,384,622 papers)\n",
      "  Processed batch 40/239 (400,000/2,384,622 papers)\n",
      "  Processed batch 50/239 (500,000/2,384,622 papers)\n",
      "  Processed batch 60/239 (600,000/2,384,622 papers)\n",
      "  Processed batch 70/239 (700,000/2,384,622 papers)\n",
      "  Processed batch 80/239 (800,000/2,384,622 papers)\n",
      "  Processed batch 90/239 (900,000/2,384,622 papers)\n",
      "  Processed batch 100/239 (1,000,000/2,384,622 papers)\n",
      "  Processed batch 110/239 (1,100,000/2,384,622 papers)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m batch = tfidf_matrix[start_idx:end_idx].toarray()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# partial fit\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i + \u001b[32m1\u001b[39m) == n_batches:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Processed batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m papers)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/sklearn/decomposition/_incremental_pca.py:362\u001b[39m, in \u001b[36mIncrementalPCA.partial_fit\u001b[39m\u001b[34m(self, X, y, check_input)\u001b[39m\n\u001b[32m    351\u001b[39m     mean_correction = np.sqrt(\n\u001b[32m    352\u001b[39m         (\u001b[38;5;28mself\u001b[39m.n_samples_seen_ / n_total_samples) * n_samples\n\u001b[32m    353\u001b[39m     ) * (\u001b[38;5;28mself\u001b[39m.mean_ - col_batch_mean)\n\u001b[32m    354\u001b[39m     X = np.vstack(\n\u001b[32m    355\u001b[39m         (\n\u001b[32m    356\u001b[39m             \u001b[38;5;28mself\u001b[39m.singular_values_.reshape((-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)) * \u001b[38;5;28mself\u001b[39m.components_,\n\u001b[32m   (...)\u001b[39m\u001b[32m    359\u001b[39m         )\n\u001b[32m    360\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m U, S, Vt = \u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m U, Vt = svd_flip(U, Vt, u_based_decision=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    364\u001b[39m explained_variance = S**\u001b[32m2\u001b[39m / (n_total_samples - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/scipy/_lib/_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1236\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/arxiv_case_study/.venv/lib/python3.13/site-packages/scipy/linalg/_decomp_svd.py:166\u001b[39m, in \u001b[36msvd\u001b[39m\u001b[34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[39m\n\u001b[32m    162\u001b[39m lwork = _compute_lwork(gesXd_lwork, a1.shape[\u001b[32m0\u001b[39m], a1.shape[\u001b[32m1\u001b[39m],\n\u001b[32m    163\u001b[39m                        compute_uv=compute_uv, full_matrices=full_matrices)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m u, s, v, info = \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info > \u001b[32m0\u001b[39m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m\"\u001b[39m\u001b[33mSVD did not converge\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# fit PCA on TF-IDF matrix in batches\n",
    "# this step could take ~ 3-4 hours for 2.4M papers\n",
    "\n",
    "print(\"\\nFitting Incremental PCA...\")\n",
    "print(\"This will take 3-4 hours...\\n\")\n",
    "\n",
    "# convert sparse matrix to dense in batches\n",
    "\n",
    "n_samples = tfidf_matrix.shape[0]\n",
    "n_batches = int(np.ceil(n_samples / PCA_BATCH_SIZE))\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_idx = i * PCA_BATCH_SIZE\n",
    "    end_idx = min(start_idx + PCA_BATCH_SIZE, n_samples)\n",
    "    \n",
    "    # get batch and convert to dense\n",
    "    batch = tfidf_matrix[start_idx:end_idx].toarray()\n",
    "    \n",
    "    # partial fit\n",
    "    pca.partial_fit(batch)\n",
    "    \n",
    "    if (i + 1) % 10 == 0 or (i + 1) == n_batches:\n",
    "        print(f\"  Processed batch {i+1}/{n_batches} ({end_idx:,}/{n_samples:,} papers)\")\n",
    "\n",
    "print(\"\\n✓ PCA fitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb268c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming data to reduced space...\n",
      "  Transformed batch 10/239\n",
      "  Transformed batch 20/239\n",
      "  Transformed batch 30/239\n",
      "  Transformed batch 40/239\n",
      "  Transformed batch 50/239\n",
      "  Transformed batch 60/239\n",
      "  Transformed batch 70/239\n",
      "  Transformed batch 80/239\n",
      "  Transformed batch 90/239\n",
      "  Transformed batch 100/239\n",
      "  Transformed batch 110/239\n",
      "  Transformed batch 120/239\n",
      "  Transformed batch 130/239\n",
      "  Transformed batch 140/239\n",
      "  Transformed batch 150/239\n",
      "  Transformed batch 160/239\n",
      "  Transformed batch 170/239\n",
      "  Transformed batch 180/239\n",
      "  Transformed batch 190/239\n",
      "  Transformed batch 200/239\n",
      "  Transformed batch 210/239\n",
      "  Transformed batch 220/239\n",
      "  Transformed batch 230/239\n",
      "  Transformed batch 239/239\n",
      "\n",
      "✓ Transformation complete!\n",
      "\n",
      "Reduced matrix shape: (2384622, 100)\n",
      "  Papers: 2,384,622\n",
      "  Components: 100\n",
      "  Memory: 1.78 GB\n"
     ]
    }
   ],
   "source": [
    "# transform TF-IDF matrix to PCA space\n",
    "\n",
    "print(\"\\nTransforming data to reduced space...\")\n",
    "\n",
    "# transform in batches (same as fitting)\n",
    "\n",
    "X_reduced_list = []\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_idx = i * PCA_BATCH_SIZE\n",
    "    end_idx = min(start_idx + PCA_BATCH_SIZE, n_samples)\n",
    "    \n",
    "    batch = tfidf_matrix[start_idx:end_idx].toarray()\n",
    "    batch_reduced = pca.transform(batch)\n",
    "    X_reduced_list.append(batch_reduced)\n",
    "    \n",
    "    if (i + 1) % 10 == 0 or (i + 1) == n_batches:\n",
    "        print(f\"  Transformed batch {i+1}/{n_batches}\")\n",
    "\n",
    "# combine all batches\n",
    "\n",
    "X_reduced = np.vstack(X_reduced_list)\n",
    "\n",
    "print(\"\\n✓ Transformation complete!\")\n",
    "print(f\"\\nReduced matrix shape: {X_reduced.shape}\")\n",
    "print(f\"  Papers: {X_reduced.shape[0]:,}\")\n",
    "print(f\"  Components: {X_reduced.shape[1]}\")\n",
    "print(f\"  Memory: {X_reduced.nbytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1773930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Analysis:\n",
      "  Total variance explained: 10.96%\n",
      "  First 10 components: 2.89%\n",
      "  First 25 components: 5.04%\n",
      "  First 50 components: 7.51%\n",
      "\n",
      "Individual component variance:\n",
      "  PC1: 0.59%\n",
      "  PC2: 0.42%\n",
      "  PC3: 0.35%\n"
     ]
    }
   ],
   "source": [
    "# analyze how much variance we preserved\n",
    "\n",
    "variance_explained = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(variance_explained)\n",
    "\n",
    "print(f\"Variance Analysis:\")\n",
    "print(f\"  Total variance explained: {cumulative_variance[-1]*100:.2f}%\")\n",
    "print(f\"  First 10 components: {cumulative_variance[9]*100:.2f}%\")\n",
    "print(f\"  First 25 components: {cumulative_variance[24]*100:.2f}%\")\n",
    "print(f\"  First 50 components: {cumulative_variance[49]*100:.2f}%\")\n",
    "print(f\"\\nIndividual component variance:\")\n",
    "print(f\"  PC1: {variance_explained[0]*100:.2f}%\")\n",
    "print(f\"  PC2: {variance_explained[1]*100:.2f}%\")\n",
    "print(f\"  PC3: {variance_explained[2]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative variance explained\n",
    "# visually see if target of 100 components is justified\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, PCA_N_COMPONENTS + 1), cumulative_variance * 100, 'b-', linewidth=2)\n",
    "plt.axhline(y=70, color='r', linestyle='--', label='70% variance')\n",
    "plt.axhline(y=80, color='g', linestyle='--', label='80% variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained (%)')\n",
    "plt.title('PCA Variance Explained')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "\n",
    "os.makedirs('results/figures', exist_ok=True)\n",
    "plt.savefig('results/figures/pca_variance_explained.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✓ Variance plot saved to: results/figures/pca_variance_explained.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variance explained summary as readable text\n",
    "\n",
    "print(\"\\nSaving variance analysis summary...\")\n",
    "\n",
    "summary_path = 'results/pca_variance_summary.txt'\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"PCA DIMENSIONALITY REDUCTION - VARIANCE ANALYSIS\\n\")\n",
    "    f.write(f\"Analysis date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "    f.write(f\"Input dimensions: {tfidf_matrix.shape[1]:,} features\\n\")\n",
    "    f.write(f\"Output dimensions: {PCA_N_COMPONENTS} components\\n\")\n",
    "    f.write(f\"Reduction ratio: {tfidf_matrix.shape[1]/PCA_N_COMPONENTS:.1f}x\\n\\n\")\n",
    "    \n",
    "    f.write(\"VARIANCE EXPLAINED\\n\")\n",
    "    f.write(f\"Total variance preserved: {cumulative_variance[-1]*100:.2f}%\\n\")\n",
    "    f.write(f\"Variance lost: {(1-cumulative_variance[-1])*100:.2f}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"Cumulative variance by component count:\\n\")\n",
    "    milestones = [1, 5, 10, 25, 50, 75, 100]\n",
    "    for n in milestones:\n",
    "        if n <= len(cumulative_variance):\n",
    "            f.write(f\"  First {n:3d} components: {cumulative_variance[n-1]*100:5.2f}%\\n\")\n",
    "    \n",
    "    f.write(f\"\\n\")\n",
    "    f.write(\"TOP COMPONENTS (Individual Variance)\\n\")\n",
    "    f.write(f\"{'Component':<12} {'Variance %':<12} {'Cumulative %'}\\n\")\n",
    "    \n",
    "    for i in range(min(20, len(variance_explained))):\n",
    "        f.write(f\"PC{i+1:<10} {variance_explained[i]*100:>10.2f}% {cumulative_variance[i]*100:>14.2f}%\\n\")\n",
    "    \n",
    "    f.write(f\"\\n\")\n",
    "    f.write(\"INTERPRETATION NOTES\\n\")\n",
    "    f.write(f\"• First component captures {variance_explained[0]*100:.1f}% of variance\\n\")\n",
    "    f.write(f\"• First 10 components capture {cumulative_variance[9]*100:.1f}% of variance\\n\")\n",
    "    f.write(f\"• {PCA_N_COMPONENTS} components preserve {cumulative_variance[-1]*100:.1f}% of information\\n\")\n",
    "    f.write(f\"• Dimensionality reduced from {tfidf_matrix.shape[1]:,} to {PCA_N_COMPONENTS}\\n\")\n",
    "\n",
    "print(f\"✓ Saved variance summary to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef31670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save outputs\n",
    "\n",
    "# save top feature loadings per component (for later interpretation)\n",
    "print(\"\\nSaving component feature loadings...\")\n",
    "\n",
    "# get top features for first 10 components\n",
    "component_features = {}\n",
    "feature_names_array = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "for i in range(min(10, PCA_N_COMPONENTS)):\n",
    "    # get loadings for this component\n",
    "    component_loading = pca.components_[i]\n",
    "    \n",
    "    # top 20 positive loadings\n",
    "    top_positive_idx = component_loading.argsort()[-20:][::-1]\n",
    "    top_positive = [(feature_names_array[idx], component_loading[idx]) \n",
    "                    for idx in top_positive_idx]\n",
    "    \n",
    "    # top 20 negative loadings  \n",
    "    top_negative_idx = component_loading.argsort()[:20]\n",
    "    top_negative = [(feature_names_array[idx], component_loading[idx]) \n",
    "                    for idx in top_negative_idx]\n",
    "    \n",
    "    component_features[f'PC{i+1}'] = {\n",
    "        'positive': top_positive,\n",
    "        'negative': top_negative\n",
    "    }\n",
    "\n",
    "component_features_path = 'data/processed/pca_component_features.pkl'\n",
    "joblib.dump(component_features, component_features_path)\n",
    "print(f\"✓ Saved component features to: {component_features_path}\")\n",
    "\n",
    "# save reduced matrix\n",
    "\n",
    "reduced_path = 'data/processed/pca_reduced_100d.pkl'\n",
    "joblib.dump(X_reduced, reduced_path)\n",
    "print(f\"✓ Saved reduced matrix to: {reduced_path}\")\n",
    "\n",
    "# save PCA model (for potential inverse transform or analysis)\n",
    "\n",
    "pca_model_path = 'data/processed/pca_model.pkl'\n",
    "joblib.dump(pca, pca_model_path)\n",
    "print(f\"✓ Saved PCA model to: {pca_model_path}\")\n",
    "\n",
    "# save variance info\n",
    "\n",
    "variance_info = {\n",
    "    'explained_variance_ratio': variance_explained,\n",
    "    'cumulative_variance': cumulative_variance,\n",
    "    'total_variance_explained': cumulative_variance[-1]\n",
    "}\n",
    "variance_path = 'data/processed/pca_variance_info.pkl'\n",
    "joblib.dump(variance_info, variance_path)\n",
    "print(f\"✓ Saved variance info to: {variance_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify all files created successfully\n",
    "\n",
    "files_to_check = [\n",
    "    'data/processed/pca_reduced_100d.pkl',\n",
    "    'data/processed/pca_model.pkl',\n",
    "    'data/processed/pca_variance_info.pkl',\n",
    "    'data/processed/pca_component_features.pkl',      \n",
    "    'results/pca_variance_summary.txt',               \n",
    "    'results/figures/pca_variance_explained.png'\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for file_path in files_to_check:\n",
    "    if os.path.exists(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / 1024**2\n",
    "        print(f\"✓ {file_path}\")\n",
    "        print(f\"  Size: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"x Missing: {file_path}\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n✓✓✓ Success! ✓✓✓ - All files created!\")\n",
    "    \n",
    "    # quick reload test\n",
    "    X_check = joblib.load('data/processed/pca_reduced_100d.pkl')\n",
    "    pca_check = joblib.load('data/processed/pca_model.pkl')\n",
    "    \n",
    "    print(f\"\\nReload verification:\")\n",
    "    print(f\"  Matrix shape: {X_check.shape}\")\n",
    "    print(f\"  PCA components: {pca_check.n_components}\")\n",
    "    print(f\"  Variance preserved: {cumulative_variance[-1]*100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nx Error - Some files missing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
