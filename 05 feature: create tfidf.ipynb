{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899306f3",
   "metadata": {},
   "source": [
    "**PHASE 2: FEATURE ENGINEERING - TF-IDF Vectorization**\n",
    "\n",
    "Purpose: Transform clean text into numerical feature matrix  \n",
    "Input: arxiv_text_cleaned.pkl  \n",
    "Output: tfidf_matrix.pkl (2.4M × 10K sparse), tfidf_vectorizer.pkl  \n",
    "Parameters: max_features=10K, min_df=10, max_df=0.7, bigrams included  \n",
    "ML Involved: YES - TF-IDF is a feature extraction technique  \n",
    "Runtime: ~20-30 minutes  \n",
    "Key Concept: Transforms text to numbers while preserving semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9bc675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports loaded\n",
      "TF-IDF Config: max_features=1000, min_df=10, max_df=0.7\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# add project root to path for config\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import TFIDF_MAX_FEATURES, TFIDF_MIN_DF, TFIDF_MAX_DF, TFIDF_NGRAM_RANGE, RANDOM_STATE\n",
    "\n",
    "print(\"✓ All imports loaded\")\n",
    "print(f\"TF-IDF Config: max_features={TFIDF_MAX_FEATURES}, min_df={TFIDF_MIN_DF}, max_df={TFIDF_MAX_DF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4f8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2,384,622 papers\n",
      "Columns: ['id', 'title', 'abstract_clean', 'year', 'primary_category', 'all_categories', 'top_level_domain', 'num_categories', 'is_multi_category', 'has_journal', 'num_authors', 'abstract_length', 'title_length']\n",
      "\n",
      "Sample abstract (cleaned):\n",
      "a fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders all next to leading order perturbative contributions\n"
     ]
    }
   ],
   "source": [
    "# load cleaned text data\n",
    "\n",
    "df = pd.read_pickle('data/processed/arxiv_text_cleaned.pkl')\n",
    "\n",
    "print(f\"Loaded: {len(df):,} papers\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nSample abstract (cleaned):\")\n",
    "print(df['abstract_clean'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380b688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty abstracts: 0\n",
      "✓ All abstracts have content\n"
     ]
    }
   ],
   "source": [
    "# verify no empty abstracts (should be 0 after preprocessing)\n",
    "\n",
    "empty_count = (df['abstract_clean'] == '').sum()\n",
    "print(f\"Empty abstracts: {empty_count:,}\")\n",
    "\n",
    "if empty_count > 0:\n",
    "    print(f\"⚠ Warning: {empty_count} empty abstracts found\")\n",
    "    print(\"Removing them now...\")\n",
    "    df = df[df['abstract_clean'] != ''].reset_index(drop=True)\n",
    "    print(f\"Papers remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"✓ All abstracts have content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f6384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vectorizer initialized\n",
      "\n",
      "Parameters:\n",
      "  max_features: 1,000\n",
      "  min_df: 10\n",
      "  max_df: 0.7\n",
      "  ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# create TF-IDF vectorizer with config parameters\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=TFIDF_MAX_FEATURES,     # top 10K terms\n",
    "    min_df=TFIDF_MIN_DF,                 # must appear in 10+ papers\n",
    "    max_df=TFIDF_MAX_DF,                 # ignore if in >70% of papers\n",
    "    ngram_range=TFIDF_NGRAM_RANGE,       # unigrams + bigrams\n",
    "    stop_words='english',                # remove common English words\n",
    "    dtype=np.float32,                    # use float32 to save memory\n",
    "    strip_accents='unicode',             # handle special characters\n",
    "    lowercase=True                       # already should be lowercase, but ensure\n",
    ")\n",
    "\n",
    "print(\"✓ Vectorizer initialized\")\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  max_features: {TFIDF_MAX_FEATURES:,}\")\n",
    "print(f\"  min_df: {TFIDF_MIN_DF}\")\n",
    "print(f\"  max_df: {TFIDF_MAX_DF}\")\n",
    "print(f\"  ngram_range: {TFIDF_NGRAM_RANGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19aad654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer and transforming abstracts...\n",
      "This will take 20-30 minutes...\n",
      "\n",
      "\n",
      "✓ TF-IDF transformation complete!\n",
      "\n",
      "Matrix shape: (2384622, 1000)\n",
      "  Papers: 2,384,622\n",
      "  Features: 1,000\n",
      "\n",
      "Sparsity: 96.41%\n",
      "Non-zero values: 85,574,897\n",
      "Memory usage: 0.32 GB\n"
     ]
    }
   ],
   "source": [
    "# fit TF-IDF and transform abstracts to feature matrix\n",
    "# this step could be ~20-30 minutes for 2.4M papers\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer and transforming abstracts...\")\n",
    "print(\"This will take 20-30 minutes...\\n\")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['abstract_clean'])\n",
    "\n",
    "print(\"\\n✓ TF-IDF transformation complete!\")\n",
    "print(f\"\\nMatrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"  Papers: {tfidf_matrix.shape[0]:,}\")\n",
    "print(f\"  Features: {tfidf_matrix.shape[1]:,}\")\n",
    "print(f\"\\nSparsity: {(1.0 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(f\"Non-zero values: {tfidf_matrix.nnz:,}\")\n",
    "print(f\"Memory usage: {tfidf_matrix.data.nbytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4042d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features extracted: 1,000\n",
      "\n",
      "First 20 features:\n",
      "['ability' 'able' 'absorption' 'access' 'account' 'accretion' 'accuracy'\n",
      " 'accurate' 'achieve' 'achieved' 'achieves' 'action' 'active' 'activity'\n",
      " 'adaptive' 'addition' 'additional' 'additionally' 'address' 'adversarial']\n",
      "\n",
      "Last 20 features:\n",
      "['velocity' 'version' 'video' 'view' 'vision' 'visual' 'volume' 'wave'\n",
      " 'waves' 'way' 'weak' 'weight' 'weighted' 'wide' 'widely' 'work' 'works'\n",
      " 'world' 'years' 'zero']\n",
      "\n",
      "Sample bigrams:\n",
      "['black hole', 'dark matter', 'deep learning', 'et al', 'experimental results', 'github com', 'language models', 'large scale', 'machine learning', 'magnetic field', 'monte carlo', 'neural network', 'neural networks', 'paper propose', 'propose novel', 'proposed method', 'real time', 'real world', 'state art']\n"
     ]
    }
   ],
   "source": [
    "# get feature names (the actual words/bigrams)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Total features extracted: {len(feature_names):,}\")\n",
    "print(f\"\\nFirst 20 features:\")\n",
    "print(feature_names[:20])\n",
    "print(f\"\\nLast 20 features:\")\n",
    "print(feature_names[-20:])\n",
    "print(f\"\\nSample bigrams:\")\n",
    "bigrams = [f for f in feature_names if ' ' in f]\n",
    "print(bigrams[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675a4dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CS - Top 15 terms:\n",
      "data, learning, model, based, models, performance, network, paper, training, methods, method, language, propose, information, approach\n",
      "\n",
      "MATH - Top 15 terms:\n",
      "prove, mathbb, paper, group, finite, space, problem, spaces, functions, result, theorem, study, results, mathcal, function\n",
      "\n",
      "COND-MAT - Top 15 terms:\n",
      "spin, magnetic, phase, temperature, quantum, field, transition, states, energy, electron, state, model, density, properties, topological\n",
      "\n",
      "ASTRO-PH - Top 15 terms:\n",
      "mass, stars, star, galaxies, ray, emission, observations, stellar, galaxy, data, formation, gas, high, observed, solar\n",
      "\n",
      "PHYSICS - Top 15 terms:\n",
      "energy, optical, high, field, model, time, laser, using, based, flow, beam, electron, method, light, results\n"
     ]
    }
   ],
   "source": [
    "# see top terms for each research domain\n",
    "# this helps verify TF-IDF captured meaningful terms\n",
    "\n",
    "domains = df['top_level_domain'].value_counts().head(5).index\n",
    "\n",
    "for domain in domains:\n",
    "    # get papers in this domain\n",
    "    domain_mask = df['top_level_domain'] == domain\n",
    "    domain_indices = df[domain_mask].index\n",
    "    \n",
    "    # get TF-IDF scores for this domain\n",
    "    domain_tfidf = tfidf_matrix[domain_indices].mean(axis=0)\n",
    "    domain_tfidf_array = np.asarray(domain_tfidf).flatten()\n",
    "    \n",
    "    # get top 15 terms\n",
    "    top_indices = domain_tfidf_array.argsort()[-15:][::-1]\n",
    "    top_terms = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    print(f\"\\n{domain.upper()} - Top 15 terms:\")\n",
    "    print(\", \".join(top_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce4e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving domain top terms analysis...\n",
      "✓ Saved domain top terms to: data/processed/domain_top_terms.pkl\n",
      "✓ Saved readable text to: results/domain_top_terms.txt\n"
     ]
    }
   ],
   "source": [
    "# save top terms per domain for case study reference\n",
    "\n",
    "print(\"\\nSaving domain top terms analysis...\")\n",
    "\n",
    "domain_top_terms = {}\n",
    "\n",
    "domains = df['top_level_domain'].value_counts().head(15).index\n",
    "\n",
    "for domain in domains:\n",
    "    # get papers in this domain\n",
    "    domain_mask = df['top_level_domain'] == domain\n",
    "    domain_indices = df[domain_mask].index\n",
    "    \n",
    "    # get TF-IDF scores for this domain\n",
    "    domain_tfidf = tfidf_matrix[domain_indices].mean(axis=0)\n",
    "    domain_tfidf_array = np.asarray(domain_tfidf).flatten()\n",
    "    \n",
    "    # get top 20 terms (save more than we show)\n",
    "    top_indices = domain_tfidf_array.argsort()[-20:][::-1]\n",
    "    top_terms = [feature_names[i] for i in top_indices]\n",
    "    top_scores = [domain_tfidf_array[i] for i in top_indices]\n",
    "    \n",
    "    domain_top_terms[domain] = {\n",
    "        'terms': top_terms,\n",
    "        'scores': top_scores,\n",
    "        'paper_count': domain_mask.sum()\n",
    "    }\n",
    "\n",
    "# save to pickle\n",
    "\n",
    "domain_terms_path = 'data/processed/domain_top_terms.pkl'\n",
    "joblib.dump(domain_top_terms, domain_terms_path)\n",
    "print(f\"✓ Saved domain top terms to: {domain_terms_path}\")\n",
    "\n",
    "# also save as readable text file for easy reference\n",
    "\n",
    "txt_path = 'results/domain_top_terms.txt'\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "with open(txt_path, 'w') as f:\n",
    "    f.write(\"TOP TERMS PER RESEARCH DOMAIN\\n\")\n",
    "    f.write(\"Generated from TF-IDF analysis of 2.4M ArXiv papers\\n\")\n",
    "    f.write(\"Higher scores indicate terms that are both frequent AND distinctive\\n\")\n",
    "    f.write(f\"Total features: {len(feature_names):,}\\n\")\n",
    "    f.write(f\"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
    "    \n",
    "    for domain in domains:\n",
    "        info = domain_top_terms[domain]\n",
    "        f.write(f\"\\n{domain.upper()}\\n\")\n",
    "        f.write(f\"Papers in domain: {info['paper_count']:,}\\n\")\n",
    "        f.write(f\"Percentage of corpus: {info['paper_count']/len(df)*100:.1f}%\\n\")\n",
    "        f.write(\"Top 20 terms (by mean TF-IDF score):\\n\")\n",
    "        \n",
    "        for i, (term, score) in enumerate(zip(info['terms'], info['scores']), 1):\n",
    "            f.write(f\"  {i:2d}. {term:25s} (score: {score:.4f})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✓ Saved readable text to: {txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "067d3fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved TF-IDF matrix to: data/processed/tfidf_matrix.pkl\n",
      "✓ Saved vectorizer to: data/processed/tfidf_vectorizer.pkl\n",
      "✓ Saved paper ID mapping to: data/processed/tfidf_paper_ids.pkl\n"
     ]
    }
   ],
   "source": [
    "# save sparse matrix (efficient storage)\n",
    "\n",
    "tfidf_path = 'data/processed/tfidf_matrix.pkl'\n",
    "joblib.dump(tfidf_matrix, tfidf_path)\n",
    "print(f\"✓ Saved TF-IDF matrix to: {tfidf_path}\")\n",
    "\n",
    "# save vectorizer (need this to interpret features later)\n",
    "\n",
    "vectorizer_path = 'data/processed/tfidf_vectorizer.pkl'\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "print(f\"✓ Saved vectorizer to: {vectorizer_path}\")\n",
    "\n",
    "# save paper IDs in same order (critical for matching later!)\n",
    "\n",
    "id_mapping_path = 'data/processed/tfidf_paper_ids.pkl'\n",
    "df[['id', 'title']].to_pickle(id_mapping_path)\n",
    "print(f\"✓ Saved paper ID mapping to: {id_mapping_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81003d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ data/processed/tfidf_matrix.pkl\n",
      "  Size: 662.0 MB\n",
      "✓ data/processed/tfidf_vectorizer.pkl\n",
      "  Size: 0.0 MB\n",
      "✓ data/processed/tfidf_paper_ids.pkl\n",
      "  Size: 209.8 MB\n",
      "✓ data/processed/domain_top_terms.pkl\n",
      "  Size: 0.0 MB\n",
      "✓ results/domain_top_terms.txt\n",
      "  Size: 0.0 MB\n",
      "\n",
      "✓✓✓ Success! ✓✓✓ - All files created!\n",
      "\n",
      "Reload verification:\n",
      "  Matrix shape: (2384622, 1000)\n",
      "  Feature count: 1000\n"
     ]
    }
   ],
   "source": [
    "# verify all files created successfully\n",
    "\n",
    "files_to_check = [\n",
    "    'data/processed/tfidf_matrix.pkl',\n",
    "    'data/processed/tfidf_vectorizer.pkl',\n",
    "    'data/processed/tfidf_paper_ids.pkl',\n",
    "    'data/processed/domain_top_terms.pkl',      \n",
    "    'results/domain_top_terms.txt'              \n",
    "]\n",
    "all_good = True\n",
    "for file_path in files_to_check:\n",
    "    if os.path.exists(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / 1024**2\n",
    "        print(f\"✓ {file_path}\")\n",
    "        print(f\"  Size: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"x Missing: {file_path}\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n✓✓✓ Success! ✓✓✓ - All files created!\")\n",
    "    \n",
    "    # quick reload test\n",
    "    matrix_check = joblib.load('data/processed/tfidf_matrix.pkl')\n",
    "    vectorizer_check = joblib.load('data/processed/tfidf_vectorizer.pkl')\n",
    "    \n",
    "    print(f\"\\nReload verification:\")\n",
    "    print(f\"  Matrix shape: {matrix_check.shape}\")\n",
    "    print(f\"  Feature count: {len(vectorizer_check.get_feature_names_out())}\")\n",
    "else:\n",
    "    print(\"\\nx Error - Some files missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: check TF-IDF matrix properties\n",
    "\n",
    "print(\"TF-IDF MATRIX DIAGNOSTICS\")\n",
    "\n",
    "# load matrix\n",
    "tfidf_matrix = joblib.load('data/processed/tfidf_matrix.pkl')\n",
    "\n",
    "# Basic stats\n",
    "print(f\"\\nShape: {tfidf_matrix.shape}\")\n",
    "print(f\"Sparsity: {(1.0 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(f\"Non-zero elements: {tfidf_matrix.nnz:,}\")\n",
    "\n",
    "# check for anomalies\n",
    "print(f\"\\nMatrix statistics:\")\n",
    "print(f\"  Min value: {tfidf_matrix.data.min():.6f}\")\n",
    "print(f\"  Max value: {tfidf_matrix.data.max():.6f}\")\n",
    "print(f\"  Mean value: {tfidf_matrix.data.mean():.6f}\")\n",
    "print(f\"  Std value: {tfidf_matrix.data.std():.6f}\")\n",
    "\n",
    "# check row-wise (per paper)\n",
    "row_sums = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "print(f\"\\nPer-paper TF-IDF sums:\")\n",
    "print(f\"  Min: {row_sums.min():.6f}\")\n",
    "print(f\"  Max: {row_sums.max():.6f}\")\n",
    "print(f\"  Mean: {row_sums.mean():.6f}\")\n",
    "print(f\"  Median: {np.median(row_sums):.6f}\")\n",
    "\n",
    "# check if any rows are all zeros\n",
    "zero_rows = (row_sums == 0).sum()\n",
    "print(f\"\\nZero rows (papers with no features): {zero_rows}\")\n",
    "\n",
    "# check column-wise (per term)\n",
    "col_sums = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "print(f\"\\nPer-term TF-IDF sums:\")\n",
    "print(f\"  Min: {col_sums.min():.6f}\")\n",
    "print(f\"  Max: {col_sums.max():.6f}\")\n",
    "print(f\"  Mean: {col_sums.mean():.6f}\")\n",
    "\n",
    "# check if any columns are all zeros\n",
    "zero_cols = (col_sums == 0).sum()\n",
    "print(f\"\\nZero columns (terms never used): {zero_cols}\")\n",
    "\n",
    "# check data type\n",
    "print(f\"\\nData type: {tfidf_matrix.dtype}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
